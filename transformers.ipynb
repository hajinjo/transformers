{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Transformers 코드 필사 \n",
        "\n",
        "[Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)   \n",
        "\n",
        "\"Attention is all you need\" 논문에서 제안한 transformer 모델을 pytorch 라이브러리로 직접 구현하는 코드 필사해보기 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKkzVvF0bKfd"
      },
      "source": [
        "Transformer 모델은 크게 4가지 클래스로 구현된다.    \n",
        "- Frame\n",
        "    - frame 역할을 하는 `EncoderDecoder` 클래스\n",
        "- Input Embedding & Encoding\n",
        "    - 입력값을 벡터화하는 `Embeddings`, `PositionalEncoding`\n",
        "- Encoder & Decoder\n",
        "    - 각 6개 layer를 갖고 있는 `Encoder`, `Decoder`\n",
        "    - layer 1층을 구현한 `EncoderLayer`, `DecoderLayer`\n",
        "- Sublayer\n",
        "    - `EncoderLayer`, `DecoderLayer` 내부에서 사용되는 Sublayer 클래스인 `MultiHeadAttiontion`, `PositionwiseFeedForward`\n",
        "    - Sublayer 클래스들을 연결하는 `SublayerConnection`\n",
        "    \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_transformer.png?raw=true\" width=\"300\" height=\"400\" align=\"center\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M7dZCBohbKfe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math, copy, time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc43XaLPbKff"
      },
      "source": [
        "### Frame\n",
        "- `EncoderDecoder`\n",
        "\n",
        "\n",
        "- `Generator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NofYE60vbKff"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder-Decoder 뼈대 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder # encoder layer\n",
        "        self.decoder = decoder # decoder layer\n",
        "        self.src_embed = src_embed # embedding layer before encoder \n",
        "        self.tgt_embed = tgt_embed # embedding layer before decoder \n",
        "        self.generator = generator # classification (prediction) layer after decoder (FFNN + softmax)\n",
        "    \n",
        "    \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        src와 tgt입력을 각각 인코더, 디코더 처리해 그 결과를 반환\n",
        "        \"\"\"\n",
        "        return self.decode(self.encode(src, src_mask),\n",
        "                           src_mask,\n",
        "                           tgt,\n",
        "                           tgt_mask\n",
        "                          )\n",
        "    \n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nATJ_UGLbKff"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    vocab(총 단어의 수)에서 하나의 단어를 예측하는 linear + softmax 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO1uzWbqbKfg"
      },
      "source": [
        "### Encoder\n",
        "- `Encoder`\n",
        "- `EncoderLayer`\n",
        "- `SublayerConnection`\n",
        "\n",
        "- multi head attention & feed forward 두 개의 sublayer로 구성\n",
        "\n",
        "  - Layer Normalization : 각 input의 feature들에 대한 평균과 분산을 구해서 각 미니배치에 있는 각 input을 정규화.\n",
        "미니배치 샘플간 의존관계 없음, 배치사이즈에 영향을 많이 받지 않는 효과 \n",
        "  - Residual Connection : 잔차 연결, 기존의 학습정보를 보존해가며 학습하기 위해 원래 값 x를 더해주는 것 \n",
        "\n",
        "![스크린샷 2022-06-11 오후 10 19 01](https://user-images.githubusercontent.com/83392231/173189670-6c806ed0-09e5-45c7-81b0-f0bbbccd5f7d.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xCngukKybKfg"
      },
      "outputs": [],
      "source": [
        "def clones(module, N):\n",
        "    \"\"\"module과 동일한 구조의 레이어를 N개 생성해 ModuleList에 담아 반환\"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vjXHapUrbKfg"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    N개의 EncoderLayer를 쌓은 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size) # Custom Class\n",
        "    \n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        입력값 x, mask를 순차적으로 EncoderLayer에 입력\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bxPIxms8bKfg"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer Normalization = 각 입력값의 feature를 정규화\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.a_2 * (x-mean) / (std + self.eps) + self.b_2\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HiTiEMYbbKfg"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    입력값을 순차적으로 \n",
        "    1. layer normalization,\n",
        "    2. sublayer,\n",
        "    3. dropout,\n",
        "    4. residual connection\n",
        "    에 통과\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        residual connection을 반환\n",
        "        \"\"\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dqa6L1P2bKfh"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    2개의 sublayer로 구성된 인코더\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPqXnQeWbKfh"
      },
      "source": [
        "### Decoder\n",
        "- `Decoder`\n",
        "- `DecoderLayer`\n",
        "- 인코더에서 masked attention sublayer가 추가 된 형태 \n",
        "\n",
        "![스크린샷 2022-06-11 오후 10 22 18](https://user-images.githubusercontent.com/83392231/173189768-500655a6-6539-4997-add0-6edcd58db815.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GbIlLdGmbKfh"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    N개의 DecoderLayer를 쌓은 모델\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        입력값 x, tgt_mask와 encoder에서 전달 받은 memory, src_mask를 순차적으로 DecoderLayer에 입력 \n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4sZG4Jp4bKfh"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    3개의 sublayer로 구성된 디코더\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x : self.self_attn(x,x,x,tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x : self.self_attn(x,m,m,src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYqQDpPbKfi"
      },
      "source": [
        "### Sublayer\n",
        "- `attention` 함수\n",
        "\n",
        "![스크린샷 2022-06-11 오후 9 44 51](https://user-images.githubusercontent.com/83392231/173188513-6c1d9855-eab6-4e01-843e-17a71b48a02e.png)\n",
        "\n",
        "$$ \\sqrt{d_k} : 차원의\\ 제곱근으로\\ score가\\ 너무\\ 커져\\ gradient가\\ 작아지는\\ 현상을\\ 방지하기\\ 위해\\ 나눠줌 $$ \n",
        "\n",
        "- `MultiHeadedAttention`\n",
        "- `PositionwiseFeedForward`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OqhvNO38bKfi"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n",
        "    \n",
        "    # mask된 token 위치의 score 값을 아주 작은 값인 -1e9로 대체함 (masked token은 attention 계산을 하지 않기 위함)\n",
        "    if mask is not None:\n",
        "        scores= scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    \n",
        "    attention = torch.matmul(p_attn, value)\n",
        "    return  attention, p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7awFpLtbKfj"
      },
      "source": [
        "- `MultiHeadedAttention`\n",
        "\n",
        "헤드를 여러개 사용하는 어텐션 구조. 어텐션 결과의 정확도를 높이기 위해서 여러개의 헤드를 사용한후 그 결과값을 더하는 형태로 진행. \n",
        "\n",
        "$$ Multi-head\\ attention = concatenate(Z_1, Z_2, ... Z_8)W_0  $$\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_multihead.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mRhVIN4gbKfj"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_model // h # multi headed attention은 d_model을 head 개수로 나눠 dimesion reduction을 함.\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(dim=1) # 새로운 dimension을 (axis 1 앞에) 추가\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # query, key, value를 각각 서로 다른 linear layer에 통과시켜(=linear projection) 얻은 값은 다시 query, key, value 변수에 할당\n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
        "          for l, x in zip(self.linears, (query,key,value))]\n",
        "        \n",
        "        # attention 적용\n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "        \n",
        "        # 8개 head의 attention을 concatenate하여 마지막 linear layer에 통과\n",
        "        x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "721kJh1FbKfk"
      },
      "source": [
        "- `PositionwiseFeedForward`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V3fhZcEJbKfk"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9soPUUMbKfk"
      },
      "source": [
        "### Input Embedding & Encoding\n",
        "- `Embeddings`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "adOkIDsjbKfk"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    입력값 x (연속적인 토큰)를 지정된 vocab 사이즈의 lookup 테이블에서 d_model 사이즈의 엠베딩으로 변환\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkpngqxtbKfk"
      },
      "source": [
        "- `PositionalEncoding`\n",
        "\n",
        "트랜스포머는 입력문장에서 단어의 순서를 고려하지 않음 \n",
        "\n",
        "단어의 위치 정보를 제공하기 위해 위치별로 특정 패턴을 따르는 포지셔널 인코딩 추가 \n",
        "\n",
        "![스크린샷 2022-06-11 오후 10 42 17](https://user-images.githubusercontent.com/83392231/173190443-cc48a870-a958-4dba-a475-eb1d22c6ae20.png)\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pe.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5eMg2TYlbKfl"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    입력값 x (엠베딩 된 3차원 텐서 (nbatches, max_len, d_model))에 positional encoding을 더해 반환\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, dropout, max_len = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0,max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                        requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988d5AtSbKfl"
      },
      "source": [
        "### Finally Build Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vqBC3vCjbKfm"
      },
      "outputs": [],
      "source": [
        "def make_model(src_vocab, tgt_vocab, \n",
        "               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(\n",
        "            EncoderLayer(d_model, c(attn), c(ff), dropout),\n",
        "            N\n",
        "        ),\n",
        "        Decoder(\n",
        "            DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout),\n",
        "            N\n",
        "        ),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab)\n",
        "    )\n",
        "    \n",
        "   # 파라미터들을 xavier_uniform 분포로 초기화 \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XozR0BBLbKfm"
      },
      "outputs": [],
      "source": [
        "model = make_model(10,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ub3lTZ4RbKfn",
        "outputId": "79d02ae2-1c9c-4b45-ecc2-5602bdbf2a6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.layers.0.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.0.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.0.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.0.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.0.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.0.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.0.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.0.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.1.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.1.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.1.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.1.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.1.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.1.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.1.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.2.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.2.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.2.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.2.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.2.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.2.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.2.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.3.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.3.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.3.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.3.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.3.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.3.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.3.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.4.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.4.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.4.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.4.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.4.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.4.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.4.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "encoder.layers.5.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "encoder.layers.5.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "encoder.layers.5.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "encoder.layers.5.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "encoder.layers.5.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "encoder.layers.5.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "encoder.layers.5.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "encoder.norm.a_2 shape: torch.Size([512])\n",
            "encoder.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.0.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.0.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.0.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.0.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.0.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.0.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.0.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.0.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.1.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.1.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.1.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.1.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.1.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.1.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.1.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.1.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.2.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.2.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.2.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.2.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.2.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.2.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.2.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.2.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.3.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.3.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.3.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.3.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.3.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.3.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.3.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.3.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.4.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.4.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.4.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.4.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.4.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.4.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.4.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.4.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.5.self_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.self_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.0.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.0.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.1.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.1.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.2.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.2.bias shape: torch.Size([512])\n",
            "decoder.layers.5.src_attn.linears.3.weight shape: torch.Size([512, 512])\n",
            "decoder.layers.5.src_attn.linears.3.bias shape: torch.Size([512])\n",
            "decoder.layers.5.feed_forward.w_1.weight shape: torch.Size([2048, 512])\n",
            "decoder.layers.5.feed_forward.w_1.bias shape: torch.Size([2048])\n",
            "decoder.layers.5.feed_forward.w_2.weight shape: torch.Size([512, 2048])\n",
            "decoder.layers.5.feed_forward.w_2.bias shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.0.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.0.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.1.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.1.norm.b_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.2.norm.a_2 shape: torch.Size([512])\n",
            "decoder.layers.5.sublayer.2.norm.b_2 shape: torch.Size([512])\n",
            "decoder.norm.a_2 shape: torch.Size([512])\n",
            "decoder.norm.b_2 shape: torch.Size([512])\n",
            "src_embed.0.lut.weight shape: torch.Size([10, 512])\n",
            "src_embed.1.pe shape: torch.Size([1, 5000, 512])\n",
            "tgt_embed.0.lut.weight shape: torch.Size([10, 512])\n",
            "tgt_embed.1.pe shape: torch.Size([1, 5000, 512])\n",
            "generator.proj.weight shape: torch.Size([10, 512])\n",
            "generator.proj.bias shape: torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for name, value in model.state_dict().items():\n",
        "    print(f\"{name} shape: {value.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjyQ1934bKfo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}